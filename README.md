# ğŸ¤– Niche GPT â€” RAG Study Assistant

> *Upload your documents. Ask anything. Get precise, source-backed answers.*

[![HuggingFace Space](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-Space-yellow)](https://huggingface.co/spaces/Emet018/Niche-GPT)
[![Python](https://img.shields.io/badge/Python-3.13-blue)](https://python.org)
[![Gradio](https://img.shields.io/badge/Gradio-6.8.0-orange)](https://gradio.app)
[![Groq](https://img.shields.io/badge/Groq-LLaMA%203.3%2070B-green)](https://groq.com)

---

## What is Niche GPT?

Niche GPT is a **Retrieval-Augmented Generation (RAG)** chatbot that lets users upload their own PDF or TXT documents and ask questions about them â€” getting precise, source-cited answers grounded strictly in their uploaded content.

Think of it like a **NotebookLM alternative** â€” but engineered for precision, accuracy, and study use cases. The chatbot does not answer from general knowledge. Every answer is retrieved from the user's own documents, cited, and grounded.

---

## How It Works â€” Full Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INGESTION PIPELINE                        â”‚
â”‚                                                                   â”‚
â”‚  User Uploads PDF/TXT                                            â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  pdfplumber extracts raw text                                    â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  RecursiveCharacterTextSplitter                                  â”‚
â”‚  chunk_size=400 Â· chunk_overlap=80                               â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  HuggingFace MiniLM-L6-v2 Embeddings                            â”‚
â”‚  batch_size=64 for speed                                         â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  ChromaDB In-Memory Vector Store (per session)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RETRIEVAL PIPELINE                        â”‚
â”‚                                                                   â”‚
â”‚  User asks a question                                            â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  Dynamic K selection based on chunk count                        â”‚
â”‚  < 100 chunks  â†’ K=4  (short files)                             â”‚
â”‚  < 500 chunks  â†’ K=6  (medium files)                            â”‚
â”‚  < 1000 chunks â†’ K=8  (long files ~50 pages)                   â”‚
â”‚  1000+ chunks  â†’ K=10 (large multi-file sessions)               â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  Semantic similarity search â†’ Top K chunks retrieved             â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  Chunks injected into LLaMA 3.3 70B system prompt               â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  Answer generated Â· Sources cited Â· Returned to user            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Features

- ğŸ“ **Multi-file upload** â€” PDF and TXT support, multiple files per session
- ğŸ”’ **Session-isolated** â€” each user's documents are private, in-memory only, never stored
- â­ï¸ **Duplicate detection** â€” re-uploading the same file skips it instantly, no RAM waste
- ğŸ“Š **Dynamic K retrieval** â€” automatically scales retrieved chunks based on document size
- ğŸ¯ **Source attribution** â€” every answer cites exactly which file it came from
- âš¡ **Groq-powered** â€” sub-5 second responses using Groq's LPU inference hardware
- ğŸ§  **LLaMA 3.3 70B** â€” large reasoning model for intent-aware, depth-calibrated answers

---

## Why LLaMA 3.3 70B â€” The Model Selection Journey

Multiple models were evaluated before settling on `llama-3.3-70b-versatile`:

| Model | Size | Speed | Reasoning | Verdict |
|---|---|---|---|---|
| `llama-3.1-8b-instant` | 8B | ~1-2s | Basic | Fast but shallow â€” no intent inference, fixed answer depth |
| `llama-3.1-70b-versatile` | 70B | ~3-5s | Strong | Decommissioned by Groq |
| `llama-3.3-70b-versatile` âœ… | 70B | ~3-5s | Strongest | Current model â€” best reasoning, newer architecture |

**Why not 8B?** Small models answer *what* to say but cannot reason about *how much* to say. A student asking "briefly explain X" gets the same length response as "explain X in detail." The 70B model naturally calibrates answer depth to question intent.

**Why not a larger model?** 70B on Groq's LPU hardware responds in 3-5 seconds â€” acceptable for a study tool. Models above 70B have no meaningful accuracy gain for document Q&A tasks and significantly slower response times.

---

## Precision & Accuracy Engineering

### Chunk Configuration â€” The Core Tradeoff

Chunk size is the single most impactful RAG parameter:

| Chunk Size | Precision | RAM Usage | Best For |
|---|---|---|---|
| 200 | Very High | High | Dense technical text |
| **400** âœ… | **High** | **Medium** | **Academic documents** |
| 600 | Medium | Low | General text |
| 1000 | Low | Very Low | Summaries only |

**Chosen:** `chunk_size=400` with `chunk_overlap=80`

The 80-token overlap ensures that sentences split across chunk boundaries are never lost â€” a critical fix for academic text where a single sentence can span 60-80 words.

### Dynamic K â€” Why Fixed K Fails

A fixed `K=5` works for a 5-page document. For a 50-page document with 500 chunks, `K=5` retrieves only 1% of available content â€” statistically likely to miss the answer entirely.

Dynamic K solves this by scaling retrieval to document volume:

```
85 chunks  (2 short files)  â†’ K=4   precise, low noise
350 chunks (1 medium PDF)   â†’ K=6   broader coverage
800 chunks (50-page doc)    â†’ K=8   wide net
1200 chunks (many files)    â†’ K=10  maximum coverage
```

### Embedding Model

`all-MiniLM-L6-v2` was chosen over alternatives for the following reasons:

- **Speed:** 5x faster than `all-mpnet-base-v2` with only ~2% accuracy loss
- **Size:** 90MB â€” fits comfortably in HF Spaces free RAM
- **Batch processing:** `batch_size=64` on 16GB RAM gives ~40-50% speed improvement over default sequential embedding
- **Quality:** Trained specifically for semantic similarity tasks â€” ideal for Q&A retrieval

---

## Session Limits & Safety

| Limit | Value | Reason |
|---|---|---|
| Max files | 25 | Prevents RAM exhaustion on 16GB HF Spaces |
| Max session size | 200MB | Safe ceiling with embedding model loaded |
| Max per-file size | 20MB | Prevents single large file from consuming all RAM |
| Max output tokens | 1500 | Enough for multi-question answers without waste |
| History window | Last 6 turns | Keeps context relevant without bloating prompt |

---

## Installation & Local Setup

```bash
git clone https://github.com/CrimsonDarkrai95/Niche-GPT
cd Niche-GPT
pip install -r requirements.txt
```

Create a `groq_api.env` file:
```
GROQ_API_KEY=your_key_here
```

Get a free Groq API key at [console.groq.com](https://console.groq.com) â€” no credit card required.

```bash
python app.py
```

---

## Tech Stack

| Component | Technology |
|---|---|
| LLM | LLaMA 3.3 70B via Groq API |
| Embeddings | HuggingFace MiniLM-L6-v2 |
| Vector Store | ChromaDB (in-memory) |
| PDF Parsing | pdfplumber |
| Text Splitting | LangChain RecursiveCharacterTextSplitter |
| Frontend | Gradio 6.8.0 |
| Hosting | HuggingFace Spaces (2 vCPU, 16GB RAM) |

---

## Architecture Decisions

**Why ChromaDB in-memory vs persistent?**
User privacy. Documents exist only for the session duration. No data is written to disk, no cross-session contamination is possible.

**Why Gradio over a custom frontend?**
Gradio runs natively on HuggingFace Spaces with zero infrastructure overhead. A React/Three.js frontend would require a separate host (Vercel) and an API layer â€” unnecessary complexity for the current scope.

**Why pdfplumber over PyPDF2?**
pdfplumber handles complex PDF layouts, tables, and multi-column academic papers significantly better than PyPDF2, which frequently garbles text extraction from research PDFs.

---

*Built with the goal of making dense academic content accessible â€” one question at a time.*