"""
Niche GPT â€” RAG Chatbot
Uses Groq (free Llama 3) + ChromaDB + Gradio 3
"""

import os
import gradio as gr
from pathlib import Path
from dotenv import load_dotenv
from groq import Groq
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from chromadb.utils import embedding_functions
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# â”€â”€ Load API key â€” works both locally and on Railway â”€â”€
load_dotenv("groq_api.env")  # local only, ignored on Railway
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
DATA_FOLDER  = os.getenv("DATA_FOLDER", "./data")
PORT         = int(os.getenv("PORT", 7860))  # Railway sets PORT automatically

if not GROQ_API_KEY:
    print("âš ï¸  GROQ_API_KEY not found.")

client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LOAD DOCUMENTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_documents(folder: str) -> list:
    docs     = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    path     = Path(folder)

    if not path.exists():
        path.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ data/ folder created.")
        return docs

    for filepath in sorted(path.iterdir()):
        text = ""

        if filepath.suffix.lower() == ".txt":
            for enc in ["utf-8", "utf-16", "cp1252", "latin-1"]:
                try:
                    text = filepath.read_text(encoding=enc, errors="ignore")
                    break
                except Exception:
                    continue

        elif filepath.suffix.lower() == ".pdf":
            try:
                import pdfplumber
                with pdfplumber.open(filepath) as pdf:
                    text = "\n".join(p.extract_text() or "" for p in pdf.pages)
            except ImportError:
                print("âš ï¸  pdfplumber not installed.")
            except Exception as e:
                print(f"âš ï¸  Could not read {filepath.name}: {e}")

        if not text.strip():
            continue

        for chunk in splitter.split_text(text):
            docs.append(Document(
                page_content=chunk,
                metadata={"source": filepath.name}
            ))
        print(f"  âœ” Loaded: {filepath.name}")

    print(f"\nâœ… {len(docs)} chunks from {len(set(d.metadata['source'] for d in docs))} file(s)")
    return docs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VECTOR STORE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nâ³ Loading embedding modelâ€¦")
ef = embedding_functions.ONNXMiniLM_L6_V2()

class ChromaEmbeddings:
    """Wrapper to make ChromaDB embeddings work with LangChain."""
    def embed_documents(self, texts):
        return ef(texts)
    def embed_query(self, text):
        return ef([text])[0]

embeddings = ChromaEmbeddings()
docs = load_documents(DATA_FOLDER)

if docs:
    db = Chroma.from_documents(
        docs, embeddings,
        collection_name="niche_db",
        persist_directory="./chroma_store"
    )
else:
    db = Chroma(
        embedding_function=embeddings,
        collection_name="niche_db",
        persist_directory="./chroma_store"
    )
    print("âš ï¸  No documents found.")

print("âœ… Ready!\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GENERATE RESPONSE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_response(query: str, history: list) -> str:
    if not query.strip():
        return ""
    if not client:
        return "âŒ No GROQ_API_KEY found."

    context_text = ""
    sources = []
    if docs:
        results      = db.similarity_search(query, k=3)
        context_text = "\n\n".join(r.page_content for r in results)
        sources      = list(dict.fromkeys(
            r.metadata.get("source", "") for r in results
            if r.metadata.get("source")
        ))

    system = (
        "You are a helpful assistant called Niche GPT. "
        "Answer using ONLY the context below. Be concise â€” 2 to 4 sentences. "
        "If the answer is not in the context, say so honestly.\n\n"
        f"--- CONTEXT ---\n{context_text}\n--- END CONTEXT ---"
        if context_text else
        "You are a helpful assistant called Niche GPT. Answer clearly and concisely."
    )

    messages = [{"role": "system", "content": system}]
    for human, bot in history[-6:]:
        if human: messages.append({"role": "user",      "content": human})
        if bot:   messages.append({"role": "assistant", "content": bot})
    messages.append({"role": "user", "content": query})

    try:
        response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=messages,
            max_tokens=400,
            temperature=0.7,
        )
        answer = response.choices[0].message.content.strip()
    except Exception as e:
        answer = f"âŒ Groq API error: {e}"

    if sources:
        answer += f"\n\nğŸ“„ *Sources: {', '.join(sources)}*"

    return answer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GRADIO UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CSS = """
@import url('https://fonts.googleapis.com/css2?family=Syne:wght@600;700&family=DM+Sans:wght@300;400;500&display=swap');

body, .gradio-container { background: #0a0a0f !important; }
footer, .built-with { display: none !important; }

#chatbox {
    background: #111118 !important;
    border: 1px solid #1e1e2e !important;
    border-radius: 14px !important;
}

.message.bot { background: #f0f0f5 !important; border-radius: 12px !important; }
.message.human { background: #e8e4ff !important; border-radius: 12px !important; }
.message.bot p, .message.bot span { color: #111111 !important; }
.message.human p, .message.human span { color: #111111 !important; }

#send-btn {
    background: linear-gradient(135deg, #7c6af7, #4fd1c5) !important;
    border: none !important;
    border-radius: 10px !important;
    color: #ffffff !important;
    font-weight: 700 !important;
}
#send-btn:hover { opacity: 0.85 !important; }
#clear-btn { border-radius: 10px !important; }

::-webkit-scrollbar { width: 4px; }
::-webkit-scrollbar-thumb { background: #1e1e2e; border-radius: 4px; }
"""

num_files  = len(set(d.metadata["source"] for d in docs)) if docs else 0
num_chunks = len(docs)

def chat(message: str, history: list):
    if not message.strip():
        return history, ""
    response = generate_response(message, history)
    return history + [(message, response)], ""

with gr.Blocks(title="Niche GPT", css=CSS) as demo:

    gr.HTML(f"""
    <div style="text-align:center; padding:36px 20px 20px; background:#0a0a0f; font-family:'DM Sans',sans-serif;">
        <div style="font-family:'Syne',sans-serif; font-size:2rem; font-weight:700; letter-spacing:-0.03em;
                    background:linear-gradient(135deg,#ffffff 30%,#4fd1c5 100%);
                    -webkit-background-clip:text; -webkit-text-fill-color:transparent; background-clip:text;">
            Niche GPT
        </div>
        <div style="font-size:0.78rem; color:#5a5a70; margin-top:6px; letter-spacing:0.08em; text-transform:uppercase;">
            Your Documents Â· Powered by Groq Â· Llama 3.1 8B
        </div>
        <div style="width:60px; height:2px; background:linear-gradient(90deg,#7c6af7,#4fd1c5); margin:14px auto 20px; border-radius:2px;"></div>
        <div style="display:flex; justify-content:center; gap:10px; flex-wrap:wrap;">
            <span style="font-size:0.73rem; color:#5a5a70; background:#111118; border:1px solid #1e1e2e; border-radius:20px; padding:4px 13px;">
                ğŸ“š <b style="color:#4fd1c5">{num_chunks}</b> chunks Â· <b style="color:#4fd1c5">{num_files}</b> file(s)
            </span>
            <span style="font-size:0.73rem; color:#5a5a70; background:#111118; border:1px solid #1e1e2e; border-radius:20px; padding:4px 13px;">
                âš¡ Groq <b style="color:#4fd1c5">Llama 3.1 8B</b> Â· Free
            </span>
            <span style="font-size:0.73rem; color:#5a5a70; background:#111118; border:1px solid #1e1e2e; border-radius:20px; padding:4px 13px;">
                ğŸ” Embeddings: <b style="color:#4fd1c5">MiniLM-L6</b>
            </span>
        </div>
    </div>
    """)

    chatbot = gr.Chatbot(
        [],
        elem_id="chatbox",
        height=460,
        show_label=False,
        avatar_images=(None, None),
    )

    with gr.Row():
        msg = gr.Textbox(
            placeholder="Ask something about your documentsâ€¦",
            show_label=False,
            elem_id="msg-box",
            lines=1,
            max_lines=6,
            scale=8,
        )
        send_btn  = gr.Button("Send â†’",  elem_id="send-btn",  scale=1, min_width=90)
        clear_btn = gr.Button("Clear",   elem_id="clear-btn", scale=1, min_width=70)

    msg.submit(chat,  [msg, chatbot], [chatbot, msg])
    send_btn.click(chat,  [msg, chatbot], [chatbot, msg])
    clear_btn.click(lambda: ([], ""), None, [chatbot, msg])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=PORT, share=False)